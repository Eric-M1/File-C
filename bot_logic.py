import requests
from time import sleep
from os import sys, path, system
from threading import Thread
from datetime import datetime
from bs4 import BeautifulSoup

URL_PHONESC = 'https://api.phonebook.space/'
HEADERS_PHONESC = {'Authorization': 'Token  69cad03c647dd216e0666a2679e71f6bb307fc88'}


def clear():
    if sys.platform == "win32":
        system("cls")
        print("[+] –ë–æ—Ç –≤–∫–ª—é—á—ë–Ω...\n")
    else:
        system("clear")
        print("[+] –ë–æ—Ç –≤–∫–ª—é—á—ë–Ω...\n")

def get_proxy_from_api():
    get_proxy = requests.get('https://api.proxyscrape.com/?request=displayproxies&proxytype=http&timeout=1900&country=all&anonymity=all&ssl=yes').text.split('\r\n')[:-1]

    get_content = BeautifulSoup(requests.get('https://free-proxy-list.net/').text, 'lxml')
    table = get_content.find('table')
    rows = table.find_all('tr')
    
    proxyP = []
    for row in rows:
        ip = row.contents[0].text
        port = row.contents[1].text
        anon = row.contents[4].text
        https = row.contents[6].text

        if https == 'no' and (anon == 'all' or anon == 'elite proxy'):
            proxyP.append(f'{ip}:{port}')

    return [*get_proxy, *proxyP]

def del_proxy():
    with open('help_files/work-proxy.txt', 'r') as f:
        lines = f.readlines()
    with open('help_files/work-proxy.txt', 'w') as f:
        f.writelines(lines[1::])

def get_number(number: str) -> str:
    number = ''.join([char for char in number if char in ('+1234567890')])

    if number.startswith('8'):
        number = ''.join(('7', number[1:]))
    if not number.startswith('+'):
        number = ''.join(('+', number))
    return number if len(number) in (10, 11, 12, 13, 14) else None

def get_data_from_avito(number: str) -> str:

    AVITO_API_BASE_URL = "https://mirror.bullshit.agency/"
    response_from_avito = requests.get(''.join((AVITO_API_BASE_URL, 'search_by_phone/', number)))

    if response_from_avito.status_code == 404:
        return 'üë®‚Äç‚öñ –ê–≤–∏—Ç–æ: –û–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\nü§∑üèª‚Äç‚ôÇÔ∏è –ê –º–æ–∂–µ—Ç –ø–æ–≤—Ç–æ—Ä–∏–º?.'
    if response_from_avito.status_code == 503:
        return '‚ö†Ô∏è –ê–≤–∏—Ç–æ: –í—Å–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –ø–æ—Ç–æ–∫–∏ –∑–∞–±–∏—Ç—ã, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ 3-5 –º–∏–Ω—É—Ç.'

    soup = BeautifulSoup(response_from_avito.text, features="lxml")
    sells_count = f"üë®‚Äç‚öñ –ê–≤–∏—Ç–æ: –ù–∞–π–¥–µ–Ω–æ{soup.title.string}"
    links = soup.find_all('a')
    hrefs = []
    for link in links:
        if link['href'].split(':')[0] == 'https':
            pass
        else:
            hrefs.append(link['href']) 

    herefs_len = len(hrefs)
    if int(herefs_len) > 20:
        del hrefs[20:]
        sellers_info = [f'üòã –ê–≤–∏—Ç–æ: –ù–∞–π–¥–µ–Ω–æ 20 –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ —Ç–µ–ª–µ—Ñ–æ–Ω—É {number}\n']
    else:
        sellers_info = [sells_count]
    for href in hrefs:
        response = requests.get(''.join((AVITO_API_BASE_URL, href)))
        soup = BeautifulSoup(response.text, features='lxml')
        try:
            sellers_info.append(f'üì¶ –ü—Ä–æ–¥–∞–ª (-—ë—Ç): {soup.h1.string}')
            sellers_info.append(f"üí∞ –¶–µ–Ω–∞: {' '.join(soup.p.string.split()[1:])}")
            sellers_info.append(f"üó∫ –ê–¥—Ä–µ—Å: {soup.select('div > p')[1].get_text(strip=True)}")
            sellers_info.append(f'üé≠ –õ–∏—Ü–æ –ø—Ä–æ–¥–∞–≤—Ü–∞: {soup.span.string}')
            sellers_info.append(f'üëΩ –ò–º—è –ø—Ä–æ–¥–∞–≤—Ü–∞: {soup.strong.string}')
            sellers_info.append(f'üîó –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä: {soup.a["href"]}\n')
        except AttributeError:
            pass
    # if len(hrefs) > 17:
    #     one_msg = sellers_info[:len(sellers_info)//2]
    #     one_msg = '\n'.join(one_msg)

    #     two_msg = sellers_info[len(sellers_info)//2:]
    #     two_msg = '\n'.join(two_msg)

    #     return one_msg, two_msg
    # elif len(hrefs) > 30:
    #     one_msg = sellers_info[:len(sellers_info)//3]
    #     one_msg = '\n'.join(one_msg)

    #     two_msg = sellers_info[len(sellers_info)//3:]
    #     two_msg = '\n'.join(two_msg)
    #     return one_msg, two_msg

    # de = '–≠—Ç–æ –≤—Å—ë!'
    if not sellers_info:
        return 'üë®‚Äç‚öñ –ê–≤–∏—Ç–æ: –ù–∏—á–µ–≥–æ –Ω–µ—Ç'
    else:
        return '\n'.join(sellers_info) 
        

def get_tags_by_number(number: str) -> str:
    respons = requests.get(''.join((URL_PHONESC, f"number/?number={number}")), headers=HEADERS_PHONESC).json()['tags']
    if not respons:
        return f'–ü–æ "{number}" –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à—ë–ª'
    else:
        return '–í–æ–∑–º–æ–∂–Ω–æ–µ –∏–º—è: '+'\n–¢–µ–≥: '.join(respons)

def get_numbers_by_name(name: str) -> str:
    try:
        respons = requests.get(''.join((URL_PHONESC, f"name/?name={name}")), headers=HEADERS_PHONESC).json()['data']
    except ValueError:
        return f'üôéüèª‚Äç‚ôÇÔ∏è - –ù–∞ "{name.title()}" –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'
    else:
        user_name = []
        if not respons:
            return f'üôéüèª‚Äç‚ôÇÔ∏è - –ù–∞ "{name.title()}" –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'
        else:
            for tag in respons:
                tags = "\n–¢–µ–≥: ".join(respons[tag])
                user_name.append(f'–ù–æ–º–µ—Ä: {tag}\n–¢–µ–≥: {tags}\n')
            return '\n'.join(user_name)


def get_data_from_num(number: str) -> str:           
    proxy = get_proxy_from_api()

    for proxy in proxy:
        try:
            print(f'[-] –ü—Ä–æ–∫—Å–∏: {proxy}') #
            proxies = {
                'http': 'http://'+proxy,
                'https': 'http://'+proxy}
            num_P = requests.post('https://htmlweb.ru/geo/api.php?json&telcod='+str(number), proxies = proxies, timeout = 7).json()#
            if num_P['limit'] == 0:
                print('[!] –õ–∏–º–∏—Ç: 0')
                continue #
            else:
                pass
            with open('help_files/work-proxy.txt', 'a') as w_proxy:
                w_proxy.write(f'{proxy}\n')
            data_num = []

            try:
                country = num_P['country']
                try:
                    data_num.append('–°—Ç—Ä–∞–Ω–∞: '+country["name"] +', '+country["fullname"])
                except KeyError:
                    if country["country_code3"] == 'UKR':
                        data_num.append('–°—Ç—Ä–∞–Ω–∞: –£–∫—Ä–∞–∏–Ω–∞')
                    else:
                        pass
                        
                try:
                    data_num.append('–ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã: '+country["country_code3"])
                except KeyError:
                    pass


                try:
                    data_num.append('–ö–æ–¥ –Ω–æ–º–µ—Ä–∞: '+str(country["telcod"]))
                except KeyError:
                    pass


                try:
                    data_num.append('–õ–æ–∫–∞—Ü–∏—è: '+country["location"])
                except KeyError:
                    pass
                

                try:
                    data_num.append('–Ø–∑—ã–∫: '+country["lang"])
                except KeyError:
                    pass
            except KeyError:
                pass

            try:
                region = num_P['region']
                endIndex = region['name'].split()
                    
                try:
                    if endIndex[1] == '–∫—Ä–∞–π':
                        data_num.append('–ö—Ä–∞–π: '+region["name"])
                except IndexError:
                    data_num.append('–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ: '+region["name"])
                except KeyError:
                    pass

                try:
                    if endIndex[1] == '–æ–±–ª–∞—Å—Ç—å':
                        data_num.append('–û–±–ª–∞—Å—Ç—å: '+region["name"])
                    else:
                        data_num.append('–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ: '+region["name"])
                except IndexError:
                    pass
                
                except KeyError:
                    pass
                try:
                    data_num.append('–û–∫—Ä—É–≥: '+region["okrug"])
                except KeyError:
                    pass
            except KeyError:
                pass

            try:
                capital = num_P['capital']
                try:
                    data_num.append('–°—Ç–æ–ª–∏—Ü–∞: '+capital["name"])
                except KeyError:
                    pass

                try:
                    data_num.append('–ö–æ–¥ –≥–æ—Ä–æ–¥–∞: +'+str(capital["telcod"]).replace(",", ", "))
                except KeyError:
                    pass            
            except KeyError:
                pass

            try:
                data = num_P['0']

                try:
                    data_num.append('–ì–æ—Ä–æ–¥: '+data["name"])
                except KeyError:
                    pass
                
                try:
                    data_num.append('–û–ø–µ—Ä–∞—Ç–æ—Ä: '+data["oper_brand"])
                except KeyError:
                    pass
                try:
                    data_num.append('–®–∏—Ä–æ—Ç–∞ / –î–æ–ª–≥–æ—Ç–∞: '+str(num_P["latitude"]) +', '+ str(num_P["longitude"]))
                except KeyError:
                    pass

                data_num.append('\n'+get_tags_by_number(number))    
                print('[+] –õ–∏–º–∏—Ç: '+str(num_P["limit"]))
            except KeyError:
                pass
            data_num.insert(0, 'üë®‚Äç‚öñ –í—Å—ë —á—Ç–æ —Å–º–æ–≥–ª–∏ –Ω–∞–π—Ç–∏:\n')
            return '\n'.join(data_num)

        except requests.exceptions.RequestException:
            continue
        except ValueError:
            return 'üë®üèª‚Äçüîß - –í–Ω–∏–º–∞–Ω–∏–µ: —Å–µ—Ä–≤–µ—Ä –æ—Ç–∫–ª–æ–Ω–∏–ª –≤–∞—à –∑–∞–ø—Ä–æ—Å.\nüëÆüèª‚Äç‚ôÇÔ∏è - –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –¥–∞–π—Ç–µ –Ω–∞–º –∑–Ω–∞—Ç—å: @FELIX4\nüíÅüèª‚Äç‚ôÇÔ∏è - –ò–ª–∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ 10-20 —Å–µ–∫'

def get_car_data(car_num: str) -> str:
    car_num = car_num.replace('–∞', 'a').replace('–≤', 'b').replace('–µ', 'e').replace('–∫', 'k').replace('–º', 'm').replace('–Ω', 'h').replace('–æ', 'o').replace('—Ä', 'p').replace('—Å', 'c').replace('—Ç', 't').replace('—É', 'y').replace('—Ö', 'x')
    result = requests.get(f'https://baza-gai.com.ua/nomer/{car_num}').text
    car_content = BeautifulSoup(result, 'html.parser')

    try:
        car_data = car_content.find('table', class_="table table-striped table-responsive tbl").text.strip().replace('\n', ' ').replace('           ', ' ').split('    ')
        car_data = [data.text.split()[::1] for data in car_content.find_all('td')]
    
        car_num_data = ['üë®‚Äç‚öñ –í—Å—ë —á—Ç–æ —Å–º–æ–≥–ª–∏ –Ω–∞–π—Ç–∏:']
        car_num_data.append(f'üìÉ –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è: {" ".join(car_data[0])}')
        car_num_data.append(f'üöå –ú–æ–¥–µ–ª—å:  {" ".join(car_data[1])}')
        car_num_data.append(f'üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {" ".join(car_data[2])}') 
        car_num_data.append(f'üìù –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: {" ".join(car_data[3])}')
        car_num_data.append(f'üó∫ –ê–¥—Ä–µ—Å: {" ".join(car_data[4])}\n')
        return '\n'.join(car_num_data)
    except AttributeError:
        return 'üë®‚Äç‚öñ –ê–≤—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω\nü§∑üèª‚Äç‚ôÇÔ∏è –ê –º–æ–∂–µ—Ç –ø–æ–≤—Ç–æ—Ä–∏–º?' 

def get_account(name: str) -> str:
    user_name = ''.join([char for char in name if char in ('_-1234567890qwertyuiopasdfghjklzxcvbnm.')])
    if not user_name:
        return '‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –∏–º—è –≤ –ê–Ω–≥–ª —Ñ–æ—Ä–º–∞—Ç–µ!'
    all_site = ['–í—Å—ë —á—Ç–æ —Å–º–æ–≥–ª–∏ –Ω–∞–π—Ç–∏:']
    file = open('help_files/link_file.txt', 'r')

    def get_response(user_name):
        for link in file:
            found = link.strip().split('www.')[1].title()
            link = link.strip()
            try:
                response = requests.get(f'{link}/{user_name}')
                if response.status_code == 200:
                    all_site.append(f'üë®‚Äç‚öñ –ê–∫–∫–∞—É–Ω—Ç –Ω–∞–π–¥–µ–Ω –≤: {found}\n–°—Å—ã–ª–∫–∞: {link}/{user_name}\n')
            except requests.exceptions.RequestException:
                pass
        

    threads = []
    for i in range(26):
        t = Thread(target=get_response, args=(user_name,))
        threads.append(t)
        t.start()

    for t in threads:
        t.join()
    
    return all_site

def get_inn_data(inn):
    URL = 'https://zachestnyibiznes.ru'
    response = requests.get(''.join((URL, '/search?query=', inn))).text
    content = BeautifulSoup(response, 'html.parser').find('table', class_='table table-bordered table-striped rwd-table')
    link = ''.join((URL, content.find('a')['href'])) 

    response_pro = requests.get(link).text
    content_pro = BeautifulSoup(response_pro, 'html.parser').find('div', class_='col-xs-12 col-md-6')

    h2_f5 = content_pro.find('h2', class_='f-s-16 f-w-400 m-b-15').text.strip()
    mt_15 = content_pro.find('div', class_='m-t-15').text.strip()
    span_address = content_pro.find('span', itemprop='address').text.strip()
    span_work = content_pro.find('span', itemprop='isicV4').nextSibling
 
    tax_data = [tax_fond.text.strip() for tax_fond in content_pro.find('div', class_='m-t-15 m-b-15').find('thead', class_='text-left').findNext('tbody').find_all('td')]
    pfr = tax_data[0].replace('\xa0\n?', '')
    reg_n = tax_data[1]
    reg_date = tax_data[2].replace('–ü–æ –¥–∞–Ω–Ω—ã–º –ø–æ—Ä—Ç–∞–ª–∞ –ó–ê–ß–ï–°–¢–ù–´–ô–ë–ò–ó–ù–ï–°', '')

    tax_mark = [tax_market.text.strip() for tax_market in content_pro.find('table', class_='m-t-5').find_all('span')]
    tax_okato = tax_mark[0]
    tax_okopf = tax_mark[1]
    tax_okfs = tax_mark[2]

    all_inn_data = []
    all_inn_data.append('\n')
    try:
        all_inn_data.append('–¶–µ–ª—å: ' + h2_f5.replace('–ü–æ –¥–∞–Ω–Ω—ã–º –ø–æ—Ä—Ç–∞–ª–∞ –ó–ê–ß–ï–°–¢–ù–´–ô–ë–ò–ó–ù–ï–°', ''))
    except AttributeError:
        pass
    try:
        all_inn_data.append(mt_15.replace(' –ü–æ –¥–∞–Ω–Ω—ã–º –ø–æ—Ä—Ç–∞–ª–∞ –ó–ê–ß–ï–°–¢–ù–´–ô–ë–ò–ó–ù–ï–°', '').replace('?', '')) 
    except AttributeError:
        pass
    try:
        all_inn_data.append('–û–∫–ø–æ: ' + content_pro.find('span', id='okpo').text)
    except AttributeError:
        pass
    try:
        all_inn_data.append('–û–∫—Ç–º–æ: ' + content_pro.find('span', id='oktmo').text)
    except AttributeError:
        pass
    try:
        all_inn_data.append('–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: ' + span_address.replace('–ü–æ –¥–∞–Ω–Ω—ã–º –ø–æ—Ä—Ç–∞–ª–∞ –ó–ê–ß–ï–°–¢–ù–´–ô–ë–ò–ó–ù–ï–°', ''))
    except AttributeError:
        pass
    try:
        all_inn_data.append('–û—Å–Ω–æ–≤–Ω–æ–π –≤–∏–¥ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: ' + span_work.replace('–ü–æ –¥–∞–Ω–Ω—ã–º –ø–æ—Ä—Ç–∞–ª–∞ –ó–ê–ß–ï–°–¢–ù–´–ô–ë–ò–ó–ù–ï–°', '').strip().title())
    except AttributeError:
        pass
    try:
        all_inn_data.append(f'\n–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤–æ –≤–Ω–µ–±—é–¥–∂–µ—Ç–Ω—ã—Ö —Ñ–æ–Ω–¥–∞—Ö:\n–§–æ–Ω–¥: {pfr}\n–†–µ–≥.–Ω–æ–º–µ—Ä: {reg_n}\n–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: {reg_date}')
    except AttributeError:
        pass
    try:
        all_inn_data.append(f'\n–ö–æ–¥—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:\n–û–∫–∞—Ç–æ: {tax_okato}\n–û–∫–æ–ø—Ñ: {tax_okopf}\n–û–∫—Ñ—Å: {tax_okfs}')
    except AttributeError:
        pass
    return all_inn_data

def get_inn(user_name):
    json = {'query': user_name.strip().title(),}
    nalog_site = requests.post('https://egrul.nalog.ru/', json=json).json()['t']
    get_response = requests.get('https://egrul.nalog.ru/search-result/'+ nalog_site)

    try:
        data_inn = ['–í—Å—ë —á—Ç–æ —Å–º–æ–≥–ª–∏ –Ω–∞–π—Ç–∏:\n']
        try:
            list_peolple = get_response.json()['rows']
        except (KeyError, AttributeError):
  	        first_choice = {'n':'0'}
        else:
            first_choice = list_peolple[0]
            inn_all_d = get_inn_data(first_choice["i"])

        if first_choice["n"].title() != user_name:
            return 'üò© –§–ò–û: –ü–æ–¥—Ö–æ–¥—è—â–∏—Ö –ª—é–¥–µ–π –ø–æ–¥ —ç—Ç–∏–º –∏–º–µ–Ω–µ–º –Ω–µ—Ç\nü§î –ê –º–æ–∂–µ—Ç –≤—ã –æ—à–∏–±–ª–∏—Å—å?'
        else:
            data_inn.append(f'–í—Å–µ–≥–æ –ª—é–¥–µ–π: {first_choice.get("tot", "–ù–∏—á–µ–≥–æ –Ω–µ—Ç")}')
            data_inn.append(f'–§–∏–æ: {first_choice.get("n", "–ù–∏—á–µ–≥–æ –Ω–µ—Ç").title()}')
            data_inn.append(f'–ò–Ω–Ω: {first_choice.get("i", "–ù–∏—á–µ–≥–æ –Ω–µ—Ç")}')
            data_inn.append(f'–û–≥—Ä–Ω–∏–ø: {first_choice.get("o", "–ù–∏—á–µ–≥–æ –Ω–µ—Ç")}')
            data_inn.append(f'–î–∞—Ç–∞ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –æ–≥—Ä–Ω–∏–ø: {first_choice.get("r", "–ù–∏—á–µ–≥–æ –Ω–µ—Ç")}')
            data_inn.append(f'–î–∞—Ç–∞ –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {first_choice.get("e", "–ù–∏—á–µ–≥–æ –Ω–µ—Ç")}')
    except IndexError:
        return 'üò© –§–ò–û: –ù–∏—á–µ–≥–æ –Ω–µ—Ç\nü§î –ê –º–æ–∂–µ—Ç –≤—ã –æ—à–∏–±–ª–∏—Å—å?'
    dat_in = '\n'.join(data_inn)
    dat_all_in = '\n'.join(inn_all_d)
    return dat_in + dat_all_in 

def get_car_photo(car_number):
    car_number_au = car_number.replace('–∞', 'a').replace('–≤', 'b').replace('–µ', 'e').replace('–∫', 'k').replace('–º', 'm').replace('–Ω', 'h').replace('–æ', 'o').replace('—Ä', 'p').replace('—Å', 'c').replace('—Ç', 't').replace('—É', 'y').replace('—Ö', 'x')
    get_c = requests.get(f"https://auto.ru/history/{car_number_au}/")
    _csrf_token = dict(get_c.cookies).get("_csrf_token", "–¢–æ–∫–µ–Ω–∞ –Ω–µ—Ç")
    if _csrf_token == '–¢–æ–∫–µ–Ω–∞ –Ω–µ—Ç':
        car_in = []
        car_in_p = []
        return car_in, car_in_p
    else:
        headers = {"x-csrf-token":_csrf_token}
        cookies = dict(_csrf_token=_csrf_token)

        response = requests.post("https://auto.ru/-/ajax/desktop/getRichVinReport/",data={"vin_or_license_plate":car_number, "isCardPage":"false", "geo_radius":300, "geo_id":[62]}, headers=headers, cookies=cookies)
        report = response.json().get('report', "–¢–æ–∫–µ–Ω–∞ –Ω–µ—Ç")
        if report == '–¢–æ–∫–µ–Ω–∞ –Ω–µ—Ç':
            car_in = []
            car_in_p = []
            return car_in, car_in_p
        else:
            car_info = []

            try:
                car_info.append(f'üèé –ú–æ–¥–µ–ª—å –º–∞—à–∏–Ω—ã: {report["header"]["title"].split(", ")[0]}')
            except KeyError:
                car_info.append("üèé –ú–æ–¥–µ–ª—å –º–∞—à–∏–Ω—ã: –ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")

            try:
                car_info.append(f'üìÜ –ì–æ–¥ –≤—ã–ø—É—Å–∫–∞: {report["pts_info"]["year"]["value"]}')
            except KeyError:
                car_info.append("üìÜ –ì–æ–¥ –≤—ã–ø—É—Å–∫–∞: –ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")

            try:
                car_info.append(f'üîã –î–≤–∏–≥–∞—Ç–µ–ª—å: {report["pts_info"]["displacement"]["value_text"]} / {report["pts_info"]["horse_power"]["value_text"]}')
            except KeyError:
                car_info.append("üîã –î–≤–∏–≥–∞—Ç–µ–ª—å: –ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")

            try:
                car_info.append(f'üé® –¶–≤–µ—Ç: {report["pts_info"]["color"]["value"]}')
            except KeyError:
                car_info.append("üé® –¶–≤–µ—Ç: –ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")

            try:
                car_info.append(f'üßØ –í–∞–∂–Ω–æ: {str(report["content"]["items"][0]["key"])}, {str(report["content"]["items"][0]["value"])}')
            except KeyError:
                car_info.append("üßØ –í–∞–∂–Ω–æ: –ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")

            try:
                car_info.append(f'‚è∞ –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {str(report["headerData"]["date"])}')
            except KeyError:
                car_info.append("‚è∞ –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: –ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")
            
            try:
                car_info.append(f'üß∞ VIN –Ω–æ–º–µ—Ä: {str(report["headerData"]["vin"])}')
            except KeyError:
                car_info.append("üß∞ VIN –Ω–æ–º–µ—Ä: –ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω")


            links = []
            URL = 'https://avto-nomer.ru'
            
            car_number_ru = car_number.replace('a', '–∞').replace('b', '–≤').replace('e', '–µ').replace('k', '–∫').replace('m', '–º').replace('h', '–Ω').replace('o', '–æ').replace('p', '—Ä').replace('c', '—Å').replace('t', '—Ç').replace('y', '—É').replace('x', '—Ö')
            res = requests.get(''.join((URL, '/gallery.php?fastsearch=', car_number_ru))).text
            co = BeautifulSoup(res, 'html.parser')
            h1_text = co.find('h1', class_='pull-left').text.strip()

            if h1_text == '–ù–∞–π–¥–µ–Ω–æ –Ω–æ–º–µ—Ä–æ–≤ 0':
                pass
            else:
                site_find = co.find('div', class_='col-md-9')
                for divs in site_find.find_all('div', class_='row margin-bottom-10'):
                    for col in divs.find_all('div', class_='col-sm-6 col-xs-12'):
                        panel_grey = col.find('div', class_='panel panel-grey')
                        panel_body = panel_grey.find('div', class_='panel-body')
                        row = panel_body.find('div', class_='row')

                        link = row.find('a')['href'].replace('/nomer', '/foto')
                        photo_st = requests.get(''.join((URL, link))).text
                        car_photo_site = BeautifulSoup(photo_st, 'html.parser')
                        links.append(car_photo_site.find('div', class_='wrapper').find('img')['src'])
                    
            return car_info, links


def all_users_f():
    users = open('help_files/users_id.txt', 'r').readlines()
    return len([user for user in users])


    